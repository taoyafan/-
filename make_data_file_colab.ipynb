{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "make_data_file_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taoyafan/Master_Graduation_Project/blob/master/make_data_file_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "W5IXhmVWC5Hp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "教程参考：\n",
        "\n",
        "[想免费用谷歌资源训练神经网络？Colab 详细使用教程](https://jinkey.ai/post/tech/xiang-mian-fei-yong-gu-ge-zi-yuan-xun-lian-shen-jing-wang-luo-colab-xiang-xi-shi-yong-jiao-cheng#toc_2)\n",
        "\n",
        "[Google Colab Free GPU Tutorial](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d)"
      ]
    },
    {
      "metadata": {
        "id": "8CdT0VtJenQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 授权登录、挂载云盘"
      ]
    },
    {
      "metadata": {
        "id": "mYV4RCpYHUW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22f7d488-6bde-4fd2-f5b5-3ca037191c6e"
      },
      "cell_type": "code",
      "source": [
        "# !pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V_wA03CoFuWq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 将数据从云盘读入虚拟机根目录"
      ]
    },
    {
      "metadata": {
        "id": "uN7X2j22fHyP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 挂载云盘后直接使用 cp 命令复制文件"
      ]
    },
    {
      "metadata": {
        "id": "yEanA2zaRWAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad0b5f94-3b75-4bdc-ddb7-d51b53668d9a"
      },
      "cell_type": "code",
      "source": [
        "file_name = \"bytecup.corpus.validation_set.txt\"\n",
        "# file_name = \"for_test.txt\"\n",
        "# file_name = \"bytecup.corpus.train.0.txt\"\n",
        "file_dir = \"drive/My\\ Drive/code\"\n",
        "\n",
        "src_file = os.path.join(file_dir, 'for_test.txt')\n",
        "os.system(\"cp %s %s\"% (src_file, file_name))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "VqfFntKvWeRg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 原始方法，不需要挂载云盘"
      ]
    },
    {
      "metadata": {
        "id": "gZOKL_F4KpIA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "显示云盘根目录的文件"
      ]
    },
    {
      "metadata": {
        "id": "1ckcHIzHMvaT",
        "colab_type": "code",
        "outputId": "ec34a3b1-77a1-478e-f844-c378c862ec27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "# for file1 in file_list:\n",
        "#       print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: code, id: 18ijhbBiNtaqpgiNwzxAf6a3W13Pc8qSa, mimeType: application/vnd.google-apps.folder\n",
            "title: Colab Notebooks, id: 1AY8s8p79p0Isoh3Za7zPf3TgLnLanjmp, mimeType: application/vnd.google-apps.folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NYHBWvYwfVDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "打开 code 文件"
      ]
    },
    {
      "metadata": {
        "id": "qg5hCNcWNTik",
        "colab_type": "code",
        "outputId": "0ed80097-6890-4673-db68-659bbaf79be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# file_list = drive.ListFile({'q': \"'18ijhbBiNtaqpgiNwzxAf6a3W13Pc8qSa' in parents and trashed=false\"}).GetList()\n",
        "# for file1 in file_list:\n",
        "#       print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: bytecup.corpus.train.0.txt, id: 18X4DEun0b3owB26YdHaPF5nzHO7j-a0N, mimeType: text/plain\n",
            "title: bytecup.corpus.validation_set.txt, id: 1N6UdW9cEm4lxcR_lW0PEDgmdWG1IVoAb, mimeType: text/plain\n",
            "title: for_test.txt, id: 1X546mxQmuWp7gpA9mzzHh3PO-nvJzz0l, mimeType: text/plain\n",
            "title: get_to_the_point, id: 1nzipK056ECQjHX8-mnKLpltqhT5P4xeU, mimeType: application/vnd.google-apps.folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tyBzeAQxffhF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "读取云盘中的文件内容"
      ]
    },
    {
      "metadata": {
        "id": "wV0Sz8WlO0F8",
        "colab_type": "code",
        "outputId": "3f06c6ac-1804-4817-dd34-34acce4f27e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def get_file_content(file_name, show = False):\n",
        "    id = [file['id'] for file in file_list if file['title'] == file_name][0]\n",
        "  \n",
        "    if id == []:\n",
        "        print(\"There are not a file named %s\"% file_name)\n",
        "        return []\n",
        "  \n",
        "    print(\"Matched file named %s, and the id is %s\"% (file_name, id))\n",
        "    file = drive.CreateFile({'id': id})\n",
        "    txt = file.GetContentString()\n",
        "    if show:\n",
        "        print(\"This is the content of file %s :\\n\\n\"% file_name)\n",
        "        print(txt)\n",
        "    \n",
        "    print(\"Read file complete, length of file is %d\" % len(txt))\n",
        "    return txt\n",
        "  \n",
        "# txt = get_file_content(file_name, show = False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched file named for_test.txt, and the id is 1X546mxQmuWp7gpA9mzzHh3PO-nvJzz0l\n",
            "Read file complete, length of file is 19331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q-GysjSBKjQW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "内容写入虚拟机"
      ]
    },
    {
      "metadata": {
        "id": "OrO8e3VEr-gy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "将内容写入根目录\n"
      ]
    },
    {
      "metadata": {
        "id": "DrqNX9tVlpOT",
        "colab_type": "code",
        "outputId": "ae0f87a3-33af-4e9f-8cdc-49332c67069b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# with open(file_name, 'w') as f:\n",
        "#     f.write(txt)\n",
        "# !ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  drive  for_test.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRe7Yv8wDNcZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "读取根目录文件，测试写入成功"
      ]
    },
    {
      "metadata": {
        "id": "Xdibi1_9l8On",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_file(str):\n",
        "    with open(str, 'r') as load_f:\n",
        "        print(\"This is the content of file %s :\\n\\n\"% str)\n",
        "        lines = load_f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            print(line)\n",
        "        print(\"\\n\\n\")\n",
        "    \n",
        "# show_file(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yw4dPrz5DTDs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "删除根目录文件（如果文件无用）"
      ]
    },
    {
      "metadata": {
        "id": "_J0xxyoGmEn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# os.remove(file_name)\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMh8LdXL9sLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 开始对数据进行预处理"
      ]
    },
    {
      "metadata": {
        "id": "uPb9bXOmKKtT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 文件路径设置"
      ]
    },
    {
      "metadata": {
        "id": "LI4cgbiVFg2Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "\n",
        "stories_dir = \"\"\n",
        "split_stories_dir = \"split_stories\"\n",
        "tokenized_stories_dir = \"tokenized_stories\"\n",
        "finished_files_dir = \"finished_files\"\n",
        "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
        "# file_names = [\"bytecup.corpus.validation_set.txt\"]\n",
        "file_names = [\"for_test.txt\"]\n",
        "# file_names = [\"bytecup.corpus.train.{}.txt\".format(i) for i in range(8,9)]\n",
        "\n",
        "if not os.path.exists(split_stories_dir): os.makedirs(split_stories_dir)\n",
        "if not os.path.exists(tokenized_stories_dir): os.makedirs(tokenized_stories_dir)\n",
        "if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
        "if not os.path.exists(chunks_dir): os.makedirs(chunks_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDQtyZ_hKFdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 将每个数据集文件拆分个多个小文件\n",
        "每个小文件包含一个故事的内容和标题，以 ID 命名"
      ]
    },
    {
      "metadata": {
        "id": "P0-C9YAaGldG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fbfc03d3-21ce-41c2-88ed-2125d1c4de6e"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "for name in file_names:\n",
        "    file_dir = os.path.join(stories_dir, name)\n",
        "    heads = []\n",
        "    desc = []\n",
        "\n",
        "    print('Opening file: %s...' % name)\n",
        "    with open(file_dir, 'r') as load_f:\n",
        "        lines = load_f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            txt_dic = json.loads(line)\n",
        "\n",
        "            split_file_dir = os.path.join(split_stories_dir, str(txt_dic['id']))\n",
        "            with open(split_file_dir, \"w\") as f:\n",
        "                f.write(txt_dic['content'].replace('.', '. '))\n",
        "                f.write(' . ')\n",
        "                if('title' in txt_dic.keys()):\n",
        "                    f.write(\"\\n\\n@title.\\n\")\n",
        "                    f.write(txt_dic['title'])\n",
        "                \n",
        "    print(\"completed\\n\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening file: for_test.txt...\n",
            "completed\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TqMYtPjxLBl1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 对每篇文章进行分词（分句）"
      ]
    },
    {
      "metadata": {
        "id": "aja5rYQoLSng",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 安装 java"
      ]
    },
    {
      "metadata": {
        "id": "gtJ5rgf4Gq-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d25ece87-5e82-465f-c6b4-3ba16e054f54"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk\n",
        "!java -version"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk is already the newest version (8u181-b13-1ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "openjdk version \"1.8.0_181\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1ubuntu0.18.04.1-b13)\n",
            "OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OiH4rB0VLXLo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 选择是否分句\n",
        "若为 DIVIDE_SENTENCE = True 则分句加分词，否则只分词"
      ]
    },
    {
      "metadata": {
        "id": "JA3wGJvJLXtg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DIVIDE_SENTENCE = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTn1OF01LqOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 只分词不分句\n",
        "先生成映射文件 mapping，再使用 PTBTokenizer 对文件进行直接操作"
      ]
    },
    {
      "metadata": {
        "id": "J-faSaLQHrwf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8aaaa936-9e3c-46db-aaa3-ea9df8a780c4"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import subprocess\n",
        "\n",
        "if not DIVIDE_SENTENCE:\n",
        "\n",
        "    time_start = time.time()\n",
        "    \n",
        "    stories = os.listdir(split_stories_dir)\n",
        "    #生成映射文件 mapping.txt\n",
        "    print(\"Making list of files to tokenize...\")\n",
        "    with open(\"mapping.txt\", \"w\") as f:\n",
        "        for s in stories:\n",
        "            f.write(\"%s \\t %s\\n\" % (os.path.join(split_stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
        "    print(\"completed\\n\")\n",
        "    \n",
        "    # Doesn't work\n",
        "    # command = ['java', 'edu.stanford.nlp.process.DocumentPreprocessor', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
        "    \n",
        "    command = ['java', 'edu.stanford.nlp.process.PTBTokenizer', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
        "    subprocess.call(command)\n",
        "    \n",
        "    # Doesn't work\n",
        "    # os.system(\"java edu.stanford.nlp.process.DocumentPreprocessor -ioFileList -preserveLines mapping.txt\")\n",
        "    \n",
        "    time_end = time.time()\n",
        "    print('Consume time:', time_end - time_start)\n",
        "    os.remove(\"mapping.txt\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making list of files to tokenize...\n",
            "completed\n",
            "\n",
            "Consume time: 0.08802556991577148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gfbXupXgLtXs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 分句加分词\n",
        "无法利用映射文件进行分词，有BUG，故只能一个一个文件分词"
      ]
    },
    {
      "metadata": {
        "id": "XQ5n9tWaHuGP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "if DIVIDE_SENTENCE:\n",
        "    \n",
        "    time_start = time.time()\n",
        "    \n",
        "    stories = os.listdir(split_stories_dir)\n",
        "    for s in stories:\n",
        "        os.system(\"java edu.stanford.nlp.process.DocumentPreprocessor -preserveLines < %s > %s\" %\n",
        "                  (os.path.join(split_stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
        "\n",
        "    time_end = time.time()\n",
        "    print('Consume time:', time_end - time_start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKNkn9ecMLM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 格式化文章并存储"
      ]
    },
    {
      "metadata": {
        "id": "Dx3N1rbkMd_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 根据文件地址读文件，返回行列表"
      ]
    },
    {
      "metadata": {
        "id": "FIpCbhHFMhQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_text_file(text_file):\n",
        "    lines = []\n",
        "    with open(text_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            lines.append(line.strip())\n",
        "        return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsazT-JuMh3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 根据文件内容获得文章和摘要"
      ]
    },
    {
      "metadata": {
        "id": "QgR0ehodMkEf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SENTENCE_START = '<s>'\n",
        "SENTENCE_END = '</s>'\n",
        "\n",
        "def get_art_abs(story_file):\n",
        "    lines = read_text_file(story_file)\n",
        "\n",
        "    # Lowercase everything\n",
        "    lines = [line.lower() for line in lines]\n",
        "\n",
        "    # Separate out article and abstract sentences\n",
        "    article_lines = []\n",
        "    titles = []\n",
        "    next_is_title = False\n",
        "    for idx,line in enumerate(lines):\n",
        "        if line == \"\":\n",
        "            continue # empty line\n",
        "        elif line.startswith(\"@title\"):\n",
        "            next_is_title = True\n",
        "        elif next_is_title:\n",
        "            titles.append(line)\n",
        "        else:\n",
        "            article_lines.append(line)\n",
        "\n",
        "    # Make article into a single string\n",
        "    article = ' '.join(article_lines)\n",
        "\n",
        "    # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
        "    title = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in titles])\n",
        "\n",
        "    return article, title"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbkjG56eMkuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 写入 bin 文件"
      ]
    },
    {
      "metadata": {
        "id": "AmvFRh4jMnB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.core.example import example_pb2\n",
        "import struct\n",
        "\n",
        "def write_to_bin(src_dir, out_file):\n",
        "    \n",
        "    with open(out_file, 'wb') as writer:\n",
        "        \n",
        "        stories = os.listdir(src_dir)\n",
        "        for idx, s in enumerate(stories):\n",
        "            story_file = os.path.join(tokenized_stories_dir, s)\n",
        "            article, title = get_art_abs(story_file)\n",
        "        #     print(\"article: \\n%s\\n\\n\"%article)\n",
        "        #     print(\"title: \\n%s\\n\" % title)\n",
        "\n",
        "            tf_example = example_pb2.Example()\n",
        "            tf_example.features.feature['article'].bytes_list.value.extend([article.encode()])\n",
        "            tf_example.features.feature['abstract'].bytes_list.value.extend([title.encode()])\n",
        "            tf_example_str = tf_example.SerializeToString()\n",
        "\n",
        "            str_len = len(tf_example_str)\n",
        "            writer.write(struct.pack('q', str_len))\n",
        "            writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
        "            \n",
        "write_to_bin(tokenized_stories_dir, os.path.join(finished_files_dir, \"test.bin\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQt_-t2UMn6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 将文件分块，每个块的文章个数为 CHUNK_SIZE"
      ]
    },
    {
      "metadata": {
        "id": "0fENII-cMqgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "08edfdaa-a9c8-451f-cabe-ac131eb0a95f"
      },
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n",
        "\n",
        "def chunk_file(set_name):\n",
        "    in_file = os.path.join(finished_files_dir, '%s.bin' % set_name)\n",
        "    with open(in_file, 'rb') as reader:\n",
        "        chunk = 0\n",
        "        finished = False\n",
        "        while not finished:\n",
        "            chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
        "            with open(chunk_fname, 'wb') as writer:\n",
        "                for _ in range(CHUNK_SIZE):\n",
        "                    len_bytes = reader.read(8)\n",
        "                    if not len_bytes:\n",
        "                        finished = True\n",
        "                        break\n",
        "                    str_len = struct.unpack('q', len_bytes)[0]\n",
        "                    example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
        "                    writer.write(struct.pack('q', str_len))\n",
        "                    writer.write(struct.pack('%ds' % str_len, example_str))\n",
        "                chunk += 1\n",
        "            \n",
        "def chunk_all():\n",
        "    # Chunk the data\n",
        "#     for set_name in ['train', 'val', 'test']:\n",
        "    for set_name in ['test']:\n",
        "        print(\"Splitting %s data into chunks...\" % set_name)\n",
        "        chunk_file(set_name)\n",
        "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
        "    \n",
        "# 分块\n",
        "chunk_all()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting test data into chunks...\n",
            "Saved chunked data in finished_files/chunked\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jHqB-wdoYUAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "59368b94-c207-40d6-fead-367b0f02261d"
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.6)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 296.9 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MjbOuQKdaBUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}